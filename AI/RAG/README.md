# RAG

## LLM의 한계 (Limitations of LLMs) : 논리적 오류를 범할 수 있음
- 사실과 다른 정보를 말할 수 있음 (Hallucination)
- AI가 잘못된 정보(존재하지 않는 사실)을 진짜처럼 말할 때가 있음
- 최신 정보를 모름 (AI가 학습한 이후에 생긴 최신 뉴스나 연구 결과를 모를 수 있음)
- 감정이나 상식을 완벽하게 이해하지 못함

=> RAG 같은 기술을 사용해서 최신 정보를 검색하는 게 중요

## RAG 개념
- RAG는 **Retrieval-Augmented Generation(검색 기반 생성)**
- AI가 더 정확하고 유용한 답변을 하기 위해 외부 데이터베이스에서 정보를 검색한 후 그 정보를 활용해서 답을 만들어내는 기술
- 챗봇이 단순히 기억하고 있는 정보만 가지고 답하는 게 아니라, 인터넷이나 문서에서 최신 정보를 찾아와서 더 정확한 답을 주는 방식

## RAG 기술 종류 (kNN-LM, REALM)
1. kNN-LM (k-Nearest Neighbors Language Model)
- AI가 어떤 질문을 받으면 비슷한 사례나 문장을 데이터베이스에서 찾아서 참고한 후 답을 만드는 방식
- 작동 방식 (AI가 원래 학습한 내용 외에도 새로운 정보를 활용할 수 있어서 더 정확하고 유연한 답변을 할 수 있음)
  1. 질문을 받으면, AI가 데이터베이스에서 비슷한 문장이나 내용을 검색
  2. 검색한 내용을 바탕으로 AI가 답변을 생성
 
2. REALM (Retrieval-Augmented Language Model)
- **검색 후 답변 생성** : Google에서 개발한 RAG(Retrieval-Augmented Generation) 방식의 AI 모델
- 기존 AI 모델은 학습된 데이터만 사용해서 답변을 하지만, REALM은 다음과 같은 과정을 거쳐 답을 만들어 더 정확하고 신뢰할 수 있는 답을 만듦
- 답을 만드는 과정
  1. 질문을 받으면, AI가 관련 정보를 검색함 (AI는 데이터베이스에서 질문과 관련된 문서를 찾음)
  2. 검색한 정보를 AI가 읽고 분석함 (단순히 정보를 가져오는 게 아니라, 문맥을 이해하고 중요한 내용을 추출)
  3. 추출한 정보를 바탕으로 답변을 생성함
- REALM의 장점
  - ✅ 최신 정보를 반영할 수 있음 → 기존 LLM은 학습된 이후의 정보를 몰랐지만, REALM은 검색을 통해 최신 정보를 반영할 수 있음
  - ✅ 더 정확한 답변 제공 → 검색한 데이터를 바탕으로 답변을 생성하므로, 헛소리(hallucination)를 줄일 수 있음
  - ✅ 학습 데이터가 부족한 질문에도 대응 가능 → 기존 AI 모델은 훈련 데이터에 없는 질문에 약하지만, REALM은 검색을 통해 부족한 정보를 보완할 수 있음
 
### **REALM vs. kNN-LM 차이점**  

| 모델 | 검색 방식 | 특징 |  
|------|---------|------|  
| **REALM** | 사전 훈련된 검색 모듈이 자동으로 관련 문서를 찾음 | 검색과 답변 생성을 동시에 최적화함 |  
| **kNN-LM** | 질문과 가장 유사한 문장을 데이터베이스에서 검색함 | 기존 학습된 모델을 유지하면서, 검색된 문장을 참고하여 답변을 생성 |  

- 즉, **REALM은 검색과 답변 생성을 한꺼번에 훈련하는 방식**, **kNN-LM은 기존 모델을 유지하면서 유사한 문장을 찾아 활용하는 방식**

## RAG 다른 기술 종류
1. FiD (Fusion-in-Decoder) : 여러 개의 문서를 활용해서 더 정교한 답변을 생성하는 기술, 여러 개의 검색된 문서를 융합해서 답변 생성
2. RePlug : LLM이 잘못된 정보를 생성하려고 할 때, 검색을 통해 이를 수정하는 역할을 하는 기술
3. RETRO (Retrieval-Enhanced Transformer) : 훈련된 데이터만 사용하지 않고, 관련 정보를 실시간으로 검색해서 참고하는 Transformer 모델
4. Atlas : 사전 훈련된 LLM과 검색을 결합해서 더 정교한 답변을 생성하는 AI 모델

| 기술명 | 특징 | 장점 | 한계 |
|--------|------|------|------|
| **kNN-LM** | 질문과 가장 유사한 문장을 데이터베이스에서 검색하여 활용 | 기존 모델을 유지하면서도 검색된 문장을 참고하여 답변을 생성 | 데이터베이스의 질과 크기에 따라 성능이 좌우됨 |
| **REALM** | 사전 훈련된 검색 모듈이 자동으로 관련 문서를 찾음 | 검색과 답변 생성을 동시에 최적화하여 더 정확한 답변 가능 | 검색 과정이 추가되어 속도가 느릴 수 있음 |
| **FiD (Fusion-in-Decoder)** | 여러 개의 검색된 문서를 융합하여 답변 생성 | 정보를 종합하여 복잡한 질문에도 강함 | 검색된 문서가 많을수록 처리 비용이 커질 수 있음 |
| **RePlug** | 검색을 통해 LLM의 잘못된 정보를 수정 | 기존 모델을 유지하면서도 쉽게 적용 가능 | 검색된 문서가 부정확하면 신뢰성이 떨어질 수 있음 |
| **RETRO (Retrieval-Enhanced Transformer)** | 검색을 통해 실시간으로 정보를 활용하는 Transformer 모델 | 기존 Transformer보다 정확하고 신뢰성 있는 정보 제공 | 실시간 검색으로 인해 응답 속도가 느릴 수 있음 |
| **Atlas** | 검색과 사전 훈련된 LLM을 결합한 모델 | 더 큰 데이터를 다룰 수 있고, 사실 검증 기능이 뛰어남 | 높은 성능을 유지하려면 많은 연산 자원이 필요함 |

## 굳이 성능이 더 좋은 LLM 대신 SLM을 사용하는 이유
- ✅ 1. 빠른 처리 속도
  - LLM(GPT-4, PaLM 등)은 연산량이 많아서 속도가 느릴 수 있음
  - SLM은 가벼운 모델이라 연산 속도가 빠름 → 모바일, IoT 기기에서도 실시간 응답 가능

- ✅ 2. 비용 절감
  - LLM은 훈련과 실행에 엄청난 GPU 자원과 전력이 필요함
  - SLM은 적은 연산 자원으로도 실행 가능 → 비용 절감 효과

- ✅ 3. 로컬 실행 가능 (보안 강화)
  - LLM은 클라우드 서버를 필요로 하는 경우가 많음 → 데이터 유출 위험이 있음
  - SLM은 오프라인(로컬)에서도 실행 가능 → 보안이 중요한 환경에서 유리함
 
- ✅ 4. 특정 작업(도메인)에 최적화 가능
  - LLM은 모든 주제에 대해 학습되어 있지만, 특정 작업에 최적화되지 않음
  - SLM은 특정 도메인(예: 의료, 법률, 고객 지원 등)에 맞춰 최적화할 수 있음

- ✅ 5. 친환경적 (에너지 절약 가능)
  - LLM은 대규모 데이터 처리를 위해 많은 전력과 GPU 자원을 사용 → 환경 부담이 큼
  - SLM은 소규모 데이터로 동작 가능하여 에너지를 절약할 수 있음

# [RAG 구현 Study](https://github.com/hachuu/developGuide/blob/main/AI/RAG/azure%EC%97%B0%EB%8F%99.md)
# [MCP 기반 RAG 멀티에이전트 개발 및 ITO 활용 실무 교육](https://github.com/hachuu/developGuide/blob/main/AI/RAG/10_16-17%20%EA%B5%90%EC%9C%A1%20MCP%20%EA%B8%B0%EB%B0%98%20RAG%20%EB%A9%80%ED%8B%B0%EC%97%90%EC%9D%B4%EC%A0%84%ED%8A%B8%20%EA%B0%9C%EB%B0%9C%20%EB%B0%8F%20ITO%20%ED%99%9C%EC%9A%A9%20%EC%8B%A4%EB%AC%B4.md)
